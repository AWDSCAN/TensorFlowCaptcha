#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è®­ç»ƒå™¨æ¨¡å—ï¼ˆå‚è€ƒcaptcha_traineræ¨¡å—åŒ–è®¾è®¡ï¼‰
åŠŸèƒ½ï¼šå°è£…è®­ç»ƒé€»è¾‘ï¼Œç¡®ä¿åŠŸèƒ½å•ä¸€æ€§
"""

import tensorflow as tf
from tensorflow import keras
from core import config
from core.data_augmentation import create_augmented_dataset


class WarmupCosineDecay(keras.optimizers.schedules.LearningRateSchedule):
    """
    Warmup + ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    å‰æœŸ: çº¿æ€§å¢é•¿ï¼ˆWarmupï¼‰
    åæœŸ: ä½™å¼¦é€€ç«
    """
    
    def __init__(self, cosine_schedule, warmup_steps, warmup_lr_start):
        super().__init__()
        self.cosine_schedule = cosine_schedule
        self.warmup_steps = tf.cast(warmup_steps, tf.float32)
        self.warmup_lr_start = tf.cast(warmup_lr_start, tf.float32)
    
    def __call__(self, step):
        step = tf.cast(step, tf.float32)
        
        # Warmupé˜¶æ®µ: çº¿æ€§å¢é•¿
        warmup_lr = (
            self.warmup_lr_start + 
            (config.LEARNING_RATE - self.warmup_lr_start) * 
            (step / self.warmup_steps)
        )
        
        # ä½™å¼¦é€€ç«é˜¶æ®µ
        cosine_lr = self.cosine_schedule(step)
        
        # å‰warmup_stepsä½¿ç”¨warmup_lrï¼Œä¹‹åä½¿ç”¨cosine_lr
        return tf.cond(
            step < self.warmup_steps,
            lambda: warmup_lr,
            lambda: cosine_lr
        )
    
    def get_config(self):
        return {
            'cosine_schedule': self.cosine_schedule,
            'warmup_steps': self.warmup_steps,
            'warmup_lr_start': self.warmup_lr_start
        }


class CaptchaTrainer:
    """
    éªŒè¯ç è®­ç»ƒå™¨
    
    å‚è€ƒï¼šcaptcha_trainer/trains.pyçš„Trainsç±»
    èŒè´£ï¼š
    - ç®¡ç†è®­ç»ƒæµç¨‹
    - é…ç½®å­¦ä¹ ç‡ç­–ç•¥
    - æ‰§è¡Œæ¨¡å‹è®­ç»ƒ
    """
    
    def __init__(self, model, use_exponential_decay=True):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        å‚æ•°:
            model: Kerasæ¨¡å‹
            use_exponential_decay: æ˜¯å¦ä½¿ç”¨æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡
        """
        self.model = model
        self.use_exponential_decay = use_exponential_decay
        self.history = None
    
    def setup_learning_rate_schedule(self, train_data, batch_size):
        """
        é…ç½®å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä½™å¼¦é€€ç«ç­–ç•¥ï¼‰
        
        ä½™å¼¦é€€ç«ä¼˜åŠ¿:
        1. å‰æœŸå¿«é€Ÿæ”¶æ•›ï¼ˆå­¦ä¹ ç‡ä»é«˜åˆ°ä½ï¼‰
        2. åæœŸç²¾ç»†ä¼˜åŒ–ï¼ˆå­¦ä¹ ç‡æ¥è¿‘æœ€å°å€¼ï¼‰
        3. å‘¨æœŸæ€§å›å‡å¯è·³å‡ºå±€éƒ¨æœ€ä¼˜
        4. ä¸Focal Losså®Œç¾æ­é…
        
        å‚æ•°:
            train_data: è®­ç»ƒæ•°æ® (X, y)
            batch_size: æ‰¹æ¬¡å¤§å°
        
        è¿”å›:
            å­¦ä¹ ç‡è°ƒåº¦å™¨
        """
        print("\nğŸ”„ ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡ï¼ˆCosine Annealing with Warmupï¼‰")
        
        # è®¡ç®—æ¯ä¸ªepochçš„æ­¥æ•°
        train_images, train_labels = train_data
        steps_per_epoch = len(train_images) // batch_size
        
        # ä½™å¼¦é€€ç« + Warmup
        lr_schedule = keras.optimizers.schedules.CosineDecayRestarts(
            initial_learning_rate=config.LEARNING_RATE,
            first_decay_steps=config.COSINE_DECAY_STEPS,
            t_mul=1.5,  # æ¯ä¸ªå‘¨æœŸå¢é•¿1.5å€
            m_mul=0.9,  # æ¯ä¸ªå‘¨æœŸæœ€å¤§å­¦ä¹ ç‡è¡°å‡è‡³0.9å€
            alpha=config.COSINE_ALPHA  # æœ€å°å­¦ä¹ ç‡æ¯”ä¾‹
        )
        
        # åŒ…è£…Warmup
        if config.WARMUP_STEPS > 0:
            lr_schedule = WarmupCosineDecay(
                lr_schedule,
                warmup_steps=config.WARMUP_STEPS,
                warmup_lr_start=config.LEARNING_RATE_MIN
            )
        
        print(f"  åˆå§‹å­¦ä¹ ç‡: {config.LEARNING_RATE}")
        print(f"  æœ€å°å­¦ä¹ ç‡: {config.LEARNING_RATE_MIN}")
        print(f"  Warmupæ­¥æ•°: {config.WARMUP_STEPS}")
        print(f"  ä½™å¼¦å‘¨æœŸ: {config.COSINE_DECAY_STEPS}æ­¥")
        print(f"  æ¯è½®æ­¥æ•°: {steps_per_epoch}")
        print(f"  é¢„è®¡100kæ­¥æ—¶å­¦ä¹ ç‡: ~0.0002ï¼ˆç²¾ç»†ä¼˜åŒ–é˜¶æ®µï¼‰")
        print()
        
        return lr_schedule
    
    def recompile_with_lr_schedule(self, lr_schedule, use_enhanced_model=True):
        """
        ä½¿ç”¨æ–°çš„å­¦ä¹ ç‡è°ƒåº¦é‡æ–°ç¼–è¯‘æ¨¡å‹
        
        å‚æ•°:
            lr_schedule: å­¦ä¹ ç‡è°ƒåº¦å™¨
            use_enhanced_model: æ˜¯å¦ä½¿ç”¨å¢å¼ºæ¨¡å‹
        """
        if use_enhanced_model:
            from extras.model_enhanced import compile_model
            self.model = compile_model(
                self.model,
                use_focal_loss=True,      # å¯ç”¨Focal Loss
                focal_gamma=2.0,          # æå‡gammaåˆ°2.0
                pos_weight=3.0,
                learning_rate=lr_schedule
            )
        else:
            from core.model import compile_model
            self.model = compile_model(self.model, learning_rate=lr_schedule)
    
    def prepare_datasets(self, train_data, val_data, batch_size):
        """
        å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        
        å‚è€ƒï¼šcaptcha_trainer/utils/data.pyçš„æ•°æ®ç®¡é“
        
        å‚æ•°:
            train_data: è®­ç»ƒæ•°æ® (X, y)
            val_data: éªŒè¯æ•°æ® (X, y)
            batch_size: æ‰¹æ¬¡å¤§å°
        
        è¿”å›:
            (train_dataset, val_dataset)
        """
        train_images, train_labels = train_data
        val_images, val_labels = val_data
        
        print("åˆ›å»ºå¢å¼ºæ•°æ®é›†...")
        train_dataset = create_augmented_dataset(
            train_images, train_labels,
            batch_size=batch_size,
            training=True
        )
        val_dataset = create_augmented_dataset(
            val_images, val_labels,
            batch_size=batch_size,
            training=False
        )
        print("âœ“ æ•°æ®å¢å¼ºpipelineå·²å¯ç”¨")
        print()
        
        return train_dataset, val_dataset
    
    def print_training_strategy(self):
        """
        æ‰“å°è®­ç»ƒç­–ç•¥ä¿¡æ¯
        
        å‚è€ƒï¼šcaptcha_trainerçš„è®­ç»ƒé…ç½®è¾“å‡º
        """
        print("=" * 80)
        print("è®­ç»ƒç­–ç•¥ï¼ˆv4.1 - ä½™å¼¦é€€ç«ä¼˜åŒ–ç‰ˆï¼‰:")
        print("  ğŸ”§ æ ¸å¿ƒç­–ç•¥:")
        print("     - Step-basedéªŒè¯: æ¯300æ­¥éªŒè¯ä¸€æ¬¡")
        print("     - ä½™å¼¦é€€ç«å­¦ä¹ ç‡: 0.001 â†’ 0.00001ï¼ˆå‘¨æœŸæ€§è¡°å‡ï¼‰")
        print("     - Warmup: å‰5000æ­¥çº¿æ€§å¢é•¿")
        print("     - å¤šæ¡ä»¶ç»ˆæ­¢: å‡†ç¡®ç‡>=80% AND æŸå¤±<=0.02 AND æ­¥æ•°>=10000")
        print("     - Step-basedä¿å­˜: æ¯100æ­¥ä¿å­˜checkpoint")
        print("  ğŸ“Š æ•°æ®å¤„ç†:")
        print("     - æ•°æ®å¢å¼º: äº®åº¦Â±12% + å¯¹æ¯”åº¦85-115%")
        print("     - æ‰¹æ¬¡å¤§å°: 128")
        print("  ğŸ¯ æ¨¡å‹é…ç½®:")
        print("     - æ­£åˆ™åŒ–: BatchNorm + Dropout 0.25/0.5")
        print("     - æŸå¤±å‡½æ•°: Focal Loss (gamma=2.0) + WeightedBCE (pos_weight=3.0)")
        print("     - ä¼˜åŒ–å™¨: Adam with AMSGrad")
        print("  â±ï¸ ç»ˆæ­¢æ¡ä»¶:")
        print("     - å®Œæ•´åŒ¹é…>=80% AND æŸå¤±<=0.02 AND æ­¥æ•°>=10000")
        print("     - æˆ–è¶…è¿‡æœ€å¤§æ­¥æ•°300000")
        print("  âš¡ é¢„è®¡è®­ç»ƒæ—¶é—´: 4-6å°æ—¶ (ä½™å¼¦é€€ç«æ”¶æ•›æ›´å¿«)")
        print("=" * 80)
        print()
    
    def train(self, train_data, val_data, epochs=None, batch_size=None, callbacks=None):
        """
        æ‰§è¡Œè®­ç»ƒ
        
        å‚è€ƒï¼šcaptcha_trainer/trains.pyçš„train_process
        
        å‚æ•°:
            train_data: è®­ç»ƒæ•°æ® (X, y)
            val_data: éªŒè¯æ•°æ® (X, y)
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            callbacks: å›è°ƒå‡½æ•°åˆ—è¡¨
        
        è¿”å›:
            è®­ç»ƒå†å²
        """
        epochs = epochs or config.EPOCHS
        batch_size = batch_size or config.BATCH_SIZE
        
        # é…ç½®å­¦ä¹ ç‡ç­–ç•¥
        if self.use_exponential_decay:
            lr_schedule = self.setup_learning_rate_schedule(train_data, batch_size)
            self.recompile_with_lr_schedule(lr_schedule, use_enhanced_model=True)
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        train_images, train_labels = train_data
        val_images, val_labels = val_data
        
        print("\n" + "=" * 80)
        print(" " * 30 + "å¼€å§‹è®­ç»ƒ")
        print("=" * 80)
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_images)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_images)}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"è®­ç»ƒè½®æ•°ä¸Šé™: {epochs}")
        print(f"åˆå§‹å­¦ä¹ ç‡: {config.LEARNING_RATE}")
        print(f"ä¼˜åŒ–å™¨: Adam with AMSGrad")
        print("=" * 80)
        
        self.print_training_strategy()
        
        # å‡†å¤‡æ•°æ®é›†
        train_dataset, val_dataset = self.prepare_datasets(
            train_data, val_data, batch_size
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.history = self.model.fit(
            train_dataset,
            epochs=epochs,
            validation_data=val_dataset,
            callbacks=callbacks,
            verbose=2
        )
        
        return self.history
    
    def get_model(self):
        """è·å–è®­ç»ƒåçš„æ¨¡å‹"""
        return self.model
    
    def get_history(self):
        """è·å–è®­ç»ƒå†å²"""
        return self.history
