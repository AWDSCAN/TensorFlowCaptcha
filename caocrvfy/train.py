#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è®­ç»ƒæ¨¡å—ï¼ˆv3.0 - æ•°æ®å¢å¼ºä¼˜åŒ–ï¼‰
åŠŸèƒ½ï¼šè®­ç»ƒéªŒè¯ç è¯†åˆ«æ¨¡å‹
å‚è€ƒtrains.pyç­–ç•¥ï¼šæ•°æ®å¢å¼º + æ›´å¼ºæ­£åˆ™åŒ– + å¿«é€Ÿå­¦ä¹ ç‡è°ƒæ•´
"""

import os
import sys
import time
import tensorflow as tf
from tensorflow import keras
from core import config
from core.data_loader import CaptchaDataLoader
from core.data_augmentation import create_augmented_dataset  # æ–°å¢æ•°æ®å¢å¼º
from core import utils

# é€‰æ‹©ä½¿ç”¨å¢å¼ºç‰ˆæ¨¡å‹è¿˜æ˜¯åŸºç¡€æ¨¡å‹
USE_ENHANCED_MODEL = True  # æ”¹ä¸ºTrueä½¿ç”¨å¢å¼ºç‰ˆæ¨¡å‹

if USE_ENHANCED_MODEL:
    from extras.model_enhanced import create_enhanced_cnn_model as create_model
    from extras.model_enhanced import compile_model, print_model_summary
    print("ä½¿ç”¨å¢å¼ºç‰ˆCNNæ¨¡å‹ï¼ˆ5å±‚å·ç§¯ + BatchNorm + æ›´å¤§FCå±‚ + æ•°æ®å¢å¼ºï¼‰")
else:
    from model import create_cnn_model as create_model
    from model import compile_model, print_model_summary
    print("ä½¿ç”¨åŸºç¡€ç‰ˆCNNæ¨¡å‹ï¼ˆ3å±‚å·ç§¯ï¼‰")


def create_callbacks(model_dir=None, log_dir=None, val_data=None):
    """
    åˆ›å»ºè®­ç»ƒå›è°ƒå‡½æ•°
    
    å‚æ•°:
        model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
        val_data: éªŒè¯æ•°æ® (X, y)ï¼Œç”¨äºè®¡ç®—å®Œæ•´åŒ¹é…å‡†ç¡®ç‡
    
    è¿”å›:
        å›è°ƒå‡½æ•°åˆ—è¡¨
    """
    model_dir = model_dir or config.MODEL_DIR
    log_dir = log_dir or config.LOG_DIR
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    
    callbacks = []
    
    # æ¨¡å‹æ£€æŸ¥ç‚¹ï¼šä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆç›‘æ§val_lossæ›´å¯é ï¼‰
    checkpoint_path = os.path.join(model_dir, 'best_model.keras')
    checkpoint = keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_path,
        monitor='val_loss',  # æ”¹ä¸ºç›‘æ§æŸå¤±
        mode='min',  # æŸå¤±è¶Šå°è¶Šå¥½
        save_best_only=True,
        save_weights_only=False,
        verbose=1
    )
    callbacks.append(checkpoint)
    
    # å»¶è¿Ÿæ—©åœï¼šå‰85è½®å……åˆ†è®­ç»ƒï¼Œä¹‹åæ‰å¯ç”¨æ—©åœç›‘æ§
    class DelayedEarlyStopping(keras.callbacks.EarlyStopping):
        """å»¶è¿Ÿæ—©åœå›è°ƒï¼šåœ¨æŒ‡å®šè½®æ¬¡ä¹‹å‰ä¸è§¦å‘æ—©åœ"""
        def __init__(self, start_epoch=85, **kwargs):
            super().__init__(**kwargs)
            self.start_epoch = start_epoch
            self.delayed_mode = True  # æ ‡è®°æ˜¯å¦å¤„äºå»¶è¿Ÿæ¨¡å¼
        
        def on_epoch_end(self, epoch, logs=None):
            # åªåœ¨è¾¾åˆ°start_epochåæ‰è°ƒç”¨çˆ¶ç±»çš„æ—©åœé€»è¾‘
            if epoch >= self.start_epoch - 1:  # epochä»0å¼€å§‹ï¼Œç¬¬85è½®æ—¶epoch=84
                if self.delayed_mode:
                    # ç¬¬ä¸€æ¬¡å¯ç”¨æ—©åœæ—¶ï¼Œæ‰“å°æç¤ºä¿¡æ¯
                    print(f"\nâ° å·²è¾¾åˆ°ç¬¬{self.start_epoch}è½®ï¼Œå¯ç”¨æ—©åœç›‘æ§ï¼ˆè€å¿ƒå€¼: {self.patience}è½®ï¼‰")
                    self.delayed_mode = False
                # è°ƒç”¨çˆ¶ç±»çš„æ—©åœé€»è¾‘
                super().on_epoch_end(epoch, logs)
            # å‰85è½®å®Œå…¨è·³è¿‡æ—©åœæ£€æŸ¥
    
    early_stop = DelayedEarlyStopping(
        start_epoch=50,  # ä»ç¬¬50è½®å¼€å§‹å¯ç”¨æ—©åœ
        monitor='val_loss',
        mode='min',
        patience=35,  # 35è½®æ— æ”¹è¿›æ‰åœæ­¢ï¼ˆå¢åŠ patienceï¼‰
        verbose=1,
        restore_best_weights=True,
        min_delta=0.00005  # é™ä½é˜ˆå€¼
    )
    callbacks.append(early_stop)
    
    # TensorBoardï¼šå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    tensorboard_log_dir = os.path.join(
        log_dir,
        f'run_{time.strftime("%Y%m%d_%H%M%S")}'
    )
    tensorboard = keras.callbacks.TensorBoard(
        log_dir=tensorboard_log_dir,
        histogram_freq=1,
        write_graph=True,
        write_images=False
    )
    callbacks.append(tensorboard)
    
    # è‡ªé€‚åº”å­¦ä¹ ç‡ï¼šåŸºäºéªŒè¯æŸå¤±åŠ¨æ€è°ƒæ•´ï¼ˆç»“åˆAdamçš„è‡ªé€‚åº”ç‰¹æ€§ï¼‰
    from core.callbacks import AdaptiveLearningRate
    
    adaptive_lr = AdaptiveLearningRate(
        monitor='val_loss',
        factor=0.5,        # æ¯æ¬¡é™ä½50%
        patience=5,        # 5è½®æ— æ”¹å–„åé™ä½
        min_lr=1e-7,       # æœ€å°å­¦ä¹ ç‡
        verbose=1
    )
    callbacks.append(adaptive_lr)
    print("  âœ“ å·²å¯ç”¨AdaptiveLearningRateï¼ˆåŸºäºval_lossåŠ¨æ€è°ƒæ•´ï¼‰")
    
    # æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ï¼ˆå‚è€ƒtrains.pyï¼šæ¯10000æ­¥Ã—0.98ï¼‰
    # TF2ä½¿ç”¨ExponentialDecay Scheduleï¼Œåœ¨compile_modelä¸­è®¾ç½®
    # æ³¨æ„ï¼šè¿™é‡Œä¸å†æ·»åŠ ReduceLROnPlateauå›è°ƒï¼Œå·²ç”¨AdaptiveLearningRateæ›¿ä»£
    
    # Step-basedéªŒè¯å’Œä¿å­˜ï¼ˆå‚è€ƒtrains.pyï¼šæ¯500æ­¥éªŒè¯ï¼Œæ¯100æ­¥ä¿å­˜ï¼‰
    class StepBasedCallbacks(keras.callbacks.Callback):
        """
        Step-basedè®­ç»ƒç­–ç•¥ï¼ˆå‚è€ƒcaptcha_trainer/trains.pyï¼‰ï¼š
        - æ¯save_stepæ­¥ä¿å­˜checkpoint
        - æ¯validation_stepsæ­¥éªŒè¯
        - å¤šæ¡ä»¶ç»ˆæ­¢ï¼šaccuracy AND loss AND steps
        """
        def __init__(self, val_data, model_dir, save_step=100, validation_steps=500,
                     end_acc=0.95, end_loss=0.01, max_steps=50000):
            super().__init__()
            self.val_images, self.val_labels = val_data
            self.model_dir = model_dir
            self.save_step = save_step
            self.validation_steps = validation_steps
            self.end_acc = end_acc  # ç›®æ ‡å‡†ç¡®ç‡
            self.end_loss = end_loss  # ç›®æ ‡æŸå¤±
            self.max_steps = max_steps  # æœ€å¤§æ­¥æ•°
            self.current_step = 0
            self.best_val_acc = 0
            self.best_val_loss = float('inf')
        
        def on_batch_end(self, batch, logs=None):
            self.current_step += 1
            logs = logs or {}
            
            # æ¯save_stepæ­¥ä¿å­˜checkpoint
            if self.current_step % self.save_step == 0:
                checkpoint_path = os.path.join(self.model_dir, f'checkpoint_step_{self.current_step}.keras')
                self.model.save(checkpoint_path)
                print(f"\n  ğŸ’¾ Step {self.current_step}: ä¿å­˜checkpoint (loss={logs.get('loss', 0):.4f})")
            
            # æ¯validation_stepsæ­¥éªŒè¯
            if self.current_step % self.validation_steps == 0:
                import numpy as np
                # é‡‡æ ·1000ä¸ªéªŒè¯æ ·æœ¬
                sample_size = min(1000, len(self.val_images))
                indices = np.random.choice(len(self.val_images), sample_size, replace=False)
                sample_images = self.val_images[indices]
                sample_labels = self.val_labels[indices]
                
                # è®¡ç®—éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡
                val_results = self.model.evaluate(sample_images, sample_labels, verbose=0)
                val_loss = val_results[0]
                val_binary_acc = val_results[1]
                
                # è®¡ç®—å®Œæ•´åŒ¹é…å‡†ç¡®ç‡
                predictions = self.model.predict(sample_images, verbose=0)
                pred_texts = [utils.vector_to_text(pred) for pred in predictions]
                true_texts = [utils.vector_to_text(label) for label in sample_labels]
                full_match_acc = utils.calculate_accuracy(true_texts, pred_texts)
                
                # è·å–å½“å‰å­¦ä¹ ç‡
                try:
                    current_lr = float(self.model.optimizer.learning_rate(self.current_step))
                except:
                    try:
                        current_lr = float(self.model.optimizer.learning_rate.numpy())
                    except:
                        current_lr = 0.001
                
                print(f"\n  ğŸ“Š Step {self.current_step} éªŒè¯ç»“æœ:")
                print(f"      éªŒè¯æŸå¤±: {val_loss:.4f} | äºŒè¿›åˆ¶å‡†ç¡®ç‡: {val_binary_acc:.4f}")
                print(f"      å®Œæ•´åŒ¹é…: {full_match_acc*100:.2f}% | å­¦ä¹ ç‡: {current_lr:.6f}")
                
                # æ›´æ–°æœ€ä½³æŒ‡æ ‡
                if full_match_acc > self.best_val_acc:
                    self.best_val_acc = full_match_acc
                    print(f"      â¬† æœ€ä½³å®Œæ•´åŒ¹é…å‡†ç¡®ç‡: {self.best_val_acc*100:.2f}%")
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    print(f"      â¬‡ æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
                
                # å¤šæ¡ä»¶ç»ˆæ­¢æ£€æŸ¥ï¼ˆå‚è€ƒtrains.pyçš„achieve_condï¼‰
                achieve_accuracy = full_match_acc >= self.end_acc
                achieve_loss = val_loss <= self.end_loss
                achieve_steps = self.current_step >= 10000  # è‡³å°‘è®­ç»ƒ10000æ­¥
                over_max_steps = self.current_step > self.max_steps
                
                if (achieve_accuracy and achieve_loss and achieve_steps) or over_max_steps:
                    print(f"\n  ğŸ¯ æ»¡è¶³ç»ˆæ­¢æ¡ä»¶:")
                    print(f"      å‡†ç¡®ç‡è¾¾æ ‡: {achieve_accuracy} (>={self.end_acc:.2%})")
                    print(f"      æŸå¤±è¾¾æ ‡: {achieve_loss} (<={self.end_loss:.4f})")
                    print(f"      æ­¥æ•°è¾¾æ ‡: {achieve_steps} (>={10000})")
                    print(f"      æˆ–è¶…è¿‡æœ€å¤§æ­¥æ•°: {over_max_steps} (>{self.max_steps})")
                    print(f"\n  âœ… æå‰ç»ˆæ­¢è®­ç»ƒï¼")
                    self.model.stop_training = True
    
    callbacks.append(StepBasedCallbacks(
        val_data=val_data,
        model_dir=model_dir,
        save_step=100,  # æ¯100æ­¥ä¿å­˜
        validation_steps=500,  # æ¯500æ­¥éªŒè¯
        end_acc=0.80,  # ç›®æ ‡80%å®Œæ•´åŒ¹é…å‡†ç¡®ç‡
        end_loss=0.05,  # ç›®æ ‡æŸå¤±0.05
        max_steps=50000  # æœ€å¤š50000æ­¥
    ))
    
    # ä¿å­˜æœ€ä½³å®Œæ•´åŒ¹é…å‡†ç¡®ç‡æ¨¡å‹
    class BestFullMatchCheckpoint(keras.callbacks.Callback):
        def __init__(self, val_data, model_dir):
            super().__init__()
            self.val_images, self.val_labels = val_data
            self.best_full_match_acc = 0
            self.model_dir = model_dir
        
        def on_epoch_end(self, epoch, logs=None):
            # æ¯5è½®è®¡ç®—ä¸€æ¬¡å®Œæ•´åŒ¹é…å‡†ç¡®ç‡
            if (epoch + 1) % 5 != 0:
                return
            
            import numpy as np
            # éšæœºé‡‡æ ·2000ä¸ªéªŒè¯æ ·æœ¬
            sample_size = min(2000, len(self.val_images))
            indices = np.random.choice(len(self.val_images), sample_size, replace=False)
            sample_images = self.val_images[indices]
            sample_labels = self.val_labels[indices]
            
            predictions = self.model.predict(sample_images, verbose=0)
            pred_texts = [utils.vector_to_text(pred) for pred in predictions]
            true_texts = [utils.vector_to_text(label) for label in sample_labels]
            full_match_acc = utils.calculate_accuracy(true_texts, pred_texts)
            
            if full_match_acc > self.best_full_match_acc:
                self.best_full_match_acc = full_match_acc
                # ä¿å­˜æœ€ä½³å®Œæ•´åŒ¹é…æ¨¡å‹
                save_path = os.path.join(self.model_dir, 'best_full_match_model.keras')
                self.model.save(save_path)
                print(f"  â­ å®Œæ•´åŒ¹é…å‡†ç¡®ç‡æå‡è‡³ {full_match_acc*100:.2f}%ï¼Œæ¨¡å‹å·²ä¿å­˜ï¼")
    
    if val_data is not None:
        callbacks.append(BestFullMatchCheckpoint(val_data=val_data, model_dir=model_dir))
    
    # è®­ç»ƒè¿›åº¦æ‰“å°ï¼ˆæ¯è½®è®¡ç®—å®Œæ•´åŒ¹é…å‡†ç¡®ç‡ï¼‰
    class TrainingProgress(keras.callbacks.Callback):
        def __init__(self, val_data):
            super().__init__()
            self.val_images, self.val_labels = val_data
            self.best_full_match_acc = 0
        
        def on_epoch_end(self, epoch, logs=None):
            logs = logs or {}
            val_loss = logs.get('val_loss', 0)
            val_binary_acc = logs.get('val_binary_accuracy', 0)
            
            # è·å–å½“å‰å­¦ä¹ ç‡ï¼ˆå…¼å®¹ä¸åŒKerasç‰ˆæœ¬ï¼‰
            try:
                # å°è¯•ç›´æ¥è·å–numpyå€¼ï¼ˆTensorFlow 2.xï¼‰
                current_lr = float(self.model.optimizer.learning_rate.numpy())
            except:
                # é™çº§åˆ°backend.get_valueï¼ˆæ—§ç‰ˆæœ¬ï¼‰
                try:
                    import tensorflow.keras.backend as K
                    current_lr = float(K.get_value(self.model.optimizer.lr))
                except:
                    current_lr = 0.001  # é»˜è®¤å€¼
            
            # è®¡ç®—å®Œæ•´åŒ¹é…å‡†ç¡®ç‡ï¼ˆæ¯è½®éƒ½è®¡ç®—ï¼Œäº†è§£çœŸå®è¿›åº¦ï¼‰
            import numpy as np
            # éšæœºé‡‡æ ·1000ä¸ªéªŒè¯æ ·æœ¬è®¡ç®—ï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰
            sample_size = min(1000, len(self.val_images))
            indices = np.random.choice(len(self.val_images), sample_size, replace=False)
            sample_images = self.val_images[indices]
            sample_labels = self.val_labels[indices]
            
            predictions = self.model.predict(sample_images, verbose=0)
            pred_texts = [utils.vector_to_text(pred) for pred in predictions]
            true_texts = [utils.vector_to_text(label) for label in sample_labels]
            full_match_acc = utils.calculate_accuracy(true_texts, pred_texts)
            
            # æ‰“å°è®­ç»ƒè¿›åº¦
            print(f"\n[Epoch {epoch+1}] è®­ç»ƒæŸå¤±: {logs.get('loss', 0):.4f} | "
                  f"éªŒè¯æŸå¤±: {val_loss:.4f} | "
                  f"äºŒè¿›åˆ¶å‡†ç¡®ç‡: {val_binary_acc:.4f} | "
                  f"å®Œæ•´åŒ¹é…: {full_match_acc*100:.2f}% | "
                  f"å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # è·Ÿè¸ªæœ€ä½³å®Œæ•´åŒ¹é…å‡†ç¡®ç‡
            if full_match_acc > self.best_full_match_acc:
                self.best_full_match_acc = full_match_acc
                print(f"    â¬† å®Œæ•´åŒ¹é…å‡†ç¡®ç‡æå‡ï¼å½“å‰: {full_match_acc*100:.2f}% (å†å²æœ€ä½³: {self.best_full_match_acc*100:.2f}%)")
    
    # æ·»åŠ è®­ç»ƒè¿›åº¦å›è°ƒï¼ˆéœ€è¦éªŒè¯æ•°æ®è®¡ç®—å®Œæ•´åŒ¹é…å‡†ç¡®ç‡ï¼‰
    if val_data is not None:
        callbacks.append(TrainingProgress(val_data=val_data))
    
    return callbacks


def train_model(
    model,
    train_data,
    val_data,
    epochs=None,
    batch_size=None,
    callbacks=None,
    use_exponential_decay=True  # æ–°å¢ï¼šä½¿ç”¨æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡
):
    """
    è®­ç»ƒæ¨¡å‹
    
    å‚æ•°:
        model: Kerasæ¨¡å‹
        train_data: è®­ç»ƒæ•°æ® (X, y)
        val_data: éªŒè¯æ•°æ® (X, y)
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        callbacks: å›è°ƒå‡½æ•°åˆ—è¡¨
        use_exponential_decay: æ˜¯å¦ä½¿ç”¨æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ï¼ˆå‚è€ƒtrains.pyï¼‰
    
    è¿”å›:
        è®­ç»ƒå†å²
    """
    epochs = epochs or config.EPOCHS
    batch_size = batch_size or config.BATCH_SIZE
    
    # å¦‚æœå¯ç”¨æŒ‡æ•°è¡°å‡ï¼Œé‡æ–°ç¼–è¯‘æ¨¡å‹ï¼ˆå‚è€ƒtrains.pyç­–ç•¥ï¼‰
    if use_exponential_decay:
        print("\nğŸ”„ ä½¿ç”¨æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ï¼ˆå‚è€ƒcaptcha_trainer/trains.pyï¼‰")
        # è®¡ç®—æ¯ä¸ªepochçš„æ­¥æ•°
        train_images, train_labels = train_data
        steps_per_epoch = len(train_images) // batch_size
        
        # æŒ‡æ•°è¡°å‡ï¼šæ¯10000æ­¥Ã—0.98ï¼ˆå‚è€ƒtrains.pyï¼‰
        lr_schedule = keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=config.LEARNING_RATE,
            decay_steps=10000,  # æ¯10000æ­¥è¡°å‡
            decay_rate=0.98,    # è¡°å‡2%
            staircase=True      # é˜¶æ¢¯å¼è¡°å‡
        )
        
        # é‡æ–°ç¼–è¯‘æ¨¡å‹ä½¿ç”¨æ–°çš„å­¦ä¹ ç‡è°ƒåº¦
        if USE_ENHANCED_MODEL:
            from model_enhanced import compile_model
            model = compile_model(
                model, 
                use_focal_loss=False, 
                pos_weight=3.0,
                learning_rate=lr_schedule  # ä¼ å…¥å­¦ä¹ ç‡è°ƒåº¦
            )
        else:
            from model import compile_model
            model = compile_model(model, learning_rate=lr_schedule)
        
        print(f"  åˆå§‹å­¦ä¹ ç‡: {config.LEARNING_RATE}")
        print(f"  è¡°å‡ç­–ç•¥: æ¯10000æ­¥ Ã— 0.98")
        print(f"  æ¯è½®æ­¥æ•°: {steps_per_epoch}")
        print()
    
    train_images, train_labels = train_data
    val_images, val_labels = val_data
    
    print("\n" + "=" * 80)
    print(" " * 30 + "å¼€å§‹è®­ç»ƒ")
    print("=" * 80)
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_images)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_images)}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"è®­ç»ƒè½®æ•°ä¸Šé™: {epochs}")
    print(f"åˆå§‹å­¦ä¹ ç‡: {config.LEARNING_RATE}")
    print(f"ä¼˜åŒ–å™¨: Adam with AMSGrad")
    print("=" * 80)
    print("è®­ç»ƒç­–ç•¥ï¼ˆv4.0 - å®Œæ•´å‚è€ƒcaptcha_trainer/trains.pyï¼‰:")
    print("  ğŸ”§ æ ¸å¿ƒç­–ç•¥ï¼ˆæ¥è‡ªtest/captcha_trainerï¼‰:")
    print("     - Step-basedéªŒè¯: æ¯500æ­¥éªŒè¯ä¸€æ¬¡ï¼ˆè€Œéæ¯epochï¼‰")
    print("     - æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡: æ¯10000æ­¥ Ã— 0.98ï¼ˆé˜¶æ¢¯å¼è¡°å‡ï¼‰")
    print("     - å¤šæ¡ä»¶ç»ˆæ­¢: å‡†ç¡®ç‡>=80% AND æŸå¤±<=0.05 AND æ­¥æ•°>=10000")
    print("     - Step-basedä¿å­˜: æ¯100æ­¥ä¿å­˜checkpoint")
    print("  ğŸ“Š æ•°æ®å¤„ç†:")
    print("     - æ•°æ®å¢å¼º: äº®åº¦/å¯¹æ¯”åº¦å˜åŒ– + éšæœºå™ªå£°")
    print("     - æ‰¹æ¬¡å¤§å°: 128")
    print("  ğŸ¯ æ¨¡å‹é…ç½®:")
    print("     - æ­£åˆ™åŒ–: BatchNorm + Dropout 0.25/0.5")
    print("     - æŸå¤±å‡½æ•°: WeightedBCE (pos_weight=3.0)")
    print("     - ä¼˜åŒ–å™¨: Adam with AMSGrad")
    print("  â±ï¸ ç»ˆæ­¢æ¡ä»¶:")
    print("     - å®Œæ•´åŒ¹é…>=80% AND æŸå¤±<=0.05 AND æ­¥æ•°>=10000")
    print("     - æˆ–è¶…è¿‡æœ€å¤§æ­¥æ•°50000ï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰")
    print("=" * 80)
    print()
    
    # ä½¿ç”¨æ•°æ®å¢å¼ºåˆ›å»ºè®­ç»ƒé›†ï¼ˆå‚è€ƒtrains.pyçš„æ•°æ®é¢„å¤„ç†ç­–ç•¥ï¼‰
    print("åˆ›å»ºå¢å¼ºæ•°æ®é›†...")
    train_dataset = create_augmented_dataset(
        train_images, train_labels, 
        batch_size=batch_size, 
        training=True
    )
    val_dataset = create_augmented_dataset(
        val_images, val_labels, 
        batch_size=batch_size, 
        training=False  # éªŒè¯é›†ä¸å¢å¼º
    )
    print("âœ“ æ•°æ®å¢å¼ºpipelineå·²å¯ç”¨")
    print()
    
    # è®­ç»ƒæ¨¡å‹
    history = model.fit(
        train_dataset,  # ä½¿ç”¨å¢å¼ºåçš„Dataset
        epochs=epochs,
        validation_data=val_dataset,  # ä½¿ç”¨Dataset
        callbacks=callbacks,
        verbose=2
    )
    
    return history


def evaluate_model(model, val_data):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        val_data: éªŒè¯æ•°æ® (X, y)
    
    è¿”å›:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    val_images, val_labels = val_data
    
    print("\n" + "=" * 80)
    print(" " * 30 + "æ¨¡å‹è¯„ä¼°")
    print("=" * 80)
    
    # Kerasè¯„ä¼°
    results = model.evaluate(val_images, val_labels, verbose=0)
    
    print(f"éªŒè¯é›†æŸå¤±: {results[0]:.4f}")
    print(f"äºŒè¿›åˆ¶å‡†ç¡®ç‡: {results[1]:.4f}")
    print(f"ç²¾ç¡®ç‡: {results[2]:.4f}")
    print(f"å¬å›ç‡: {results[3]:.4f}")
    print()
    
    # å®Œæ•´åŒ¹é…å‡†ç¡®ç‡è¯„ä¼°
    print("è®¡ç®—å®Œæ•´éªŒè¯ç åŒ¹é…å‡†ç¡®ç‡...")
    predictions = model.predict(val_images, verbose=0)
    
    # è§£ç é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
    pred_texts = [utils.vector_to_text(pred) for pred in predictions]
    true_texts = [utils.vector_to_text(label) for label in val_labels]
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = utils.calculate_accuracy(true_texts, pred_texts)
    
    print(f"å®Œæ•´åŒ¹é…å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print()
    
    # æ˜¾ç¤ºç¤ºä¾‹é¢„æµ‹
    print("ç¤ºä¾‹é¢„æµ‹ï¼ˆå‰10ä¸ªï¼‰:")
    print("-" * 80)
    print(f"{'çœŸå®å€¼':<15} {'é¢„æµ‹å€¼':<15} {'åŒ¹é…':<10}")
    print("-" * 80)
    for i in range(min(10, len(true_texts))):
        match = "âœ“" if true_texts[i] == pred_texts[i] else "âœ—"
        print(f"{true_texts[i]:<15} {pred_texts[i]:<15} {match:<10}")
    print("=" * 80)
    
    return {
        'loss': results[0],
        'binary_accuracy': results[1],
        'precision': results[2],
        'recall': results[3],
        'full_match_accuracy': accuracy
    }


def save_model(model, save_path=None):
    """
    ä¿å­˜æ¨¡å‹
    
    å‚æ•°:
        model: Kerasæ¨¡å‹
        save_path: ä¿å­˜è·¯å¾„
    """
    save_path = save_path or os.path.join(config.MODEL_DIR, 'final_model.keras')
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    
    print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    # ä¿å­˜æ¨¡å‹å¤§å°
    model_size = os.path.getsize(save_path) / (1024 ** 2)
    print(f"æ¨¡å‹æ–‡ä»¶å¤§å°: {model_size:.2f} MB")


# ä¸»è®­ç»ƒæµç¨‹
def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("=" * 80)
    print(" " * 25 + "éªŒè¯ç è¯†åˆ«æ¨¡å‹è®­ç»ƒ")
    print("=" * 80)
    print()
    
    # 1. åŠ è½½æ•°æ®
    print("æ­¥éª¤ 1/5: åŠ è½½æ•°æ®")
    print("-" * 80)
    loader = CaptchaDataLoader()
    loader.load_data()
    loader.print_statistics()
    print()
    
    # 2. å‡†å¤‡æ•°æ®é›†
    print("æ­¥éª¤ 2/5: å‡†å¤‡æ•°æ®é›†")
    print("-" * 80)
    train_images, train_labels, val_images, val_labels = loader.prepare_dataset()
    print()
    
    # 3. åˆ›å»ºæ¨¡å‹
    print("æ­¥éª¤ 3/5: åˆ›å»ºæ¨¡å‹")
    print("-" * 80)
    model = create_model()
    # ä½¿ç”¨åŠ æƒBCE Lossï¼ˆæ­£ç±»æƒé‡=3.0ï¼Œè§£å†³ç±»åˆ«ä¸å¹³è¡¡ï¼šå¬å›ç‡37%â†’90%+ï¼‰
    model = compile_model(model, use_focal_loss=False, pos_weight=3.0)
    print_model_summary(model)
    print()
    
    # 4. è®­ç»ƒæ¨¡å‹
    print("æ­¥éª¤ 4/5: è®­ç»ƒæ¨¡å‹")
    print("-" * 80)
    callbacks = create_callbacks(val_data=(val_images, val_labels))
    history = train_model(
        model,
        train_data=(train_images, train_labels),
        val_data=(val_images, val_labels),
        callbacks=callbacks,
        epochs=500,  # 500è½®ä¸Šé™ï¼ˆstep-basedç»ˆæ­¢ä¼šæå‰åœæ­¢ï¼‰
        use_exponential_decay=True  # ä½¿ç”¨æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡
    )
    print()
    
    # 5. è¯„ä¼°æ¨¡å‹
    print("æ­¥éª¤ 5/5: è¯„ä¼°æ¨¡å‹")
    print("-" * 80)
    metrics = evaluate_model(model, val_data=(val_images, val_labels))
    print()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_model(model)
    
    print("\n" + "=" * 80)
    print(" " * 30 + "è®­ç»ƒå®Œæˆ")
    print("=" * 80)
    print(f"\næœ€ç»ˆéªŒè¯é›†å®Œæ•´åŒ¹é…å‡†ç¡®ç‡: {metrics['full_match_accuracy']*100:.2f}%")
    print()
    
    return model, history, metrics


if __name__ == '__main__':
    # è®¾ç½®GPUå†…å­˜å¢é•¿
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ“ æ£€æµ‹åˆ° {len(gpus)} ä¸ªGPUï¼Œå·²å¯ç”¨å†…å­˜å¢é•¿æ¨¡å¼")
        except RuntimeError as e:
            print(f"GPUè®¾ç½®é”™è¯¯: {e}")
    else:
        print("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    print()
    
    # è¿è¡Œè®­ç»ƒ
    main()
